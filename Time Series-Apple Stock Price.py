# -*- coding: utf-8 -*-
"""Time Series_Faisal Ahmad Gifari_Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c-ODtFIZWrQWW8BwAo_l8SW5onXEBQJ2

# Data Diri

- Nama : Faisal Ahmad Gifari
- Jenis Kelamin : Laki-Laki
- Pekerjaaan : Mahasiswa
- Tempat/Tanggal Lahir : Kuningan, 17 September 2002
- Username : faisal_ag_037
- email : pd-20379543@edu.jakarta.go.id
- No. Telepon : 085775063559
- Kota Domisili : Jakarta Barat
- Institusi : UIN Syarif Hidayatullah Jakarta

#Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
import tensorflow as tf

main_dataframe = pd.read_csv("//content//drive//MyDrive//Datasets//AAPL.csv")

"""# EDA"""

main_dataframe

"""Dataset yang digunakan memiliki jumlah sampel lebih dari 10000"""

main_dataframe.head(10)

main_dataframe.tail(10)

main_dataframe.describe()

main_dataframe.info()

main_dataframe.isnull().sum()

main_dataframe.set_index('Date')['Close'].plot()

main_dataframe.set_index('Date')[['Open', 'High', 'Low', 'Close', 'Volume']].plot(subplots=True)

print('test')

"""# Data Preprocessing"""

close_only = main_dataframe.filter(['Close'])

scaler = MinMaxScaler(feature_range = (0, 1))
close_scaled = scaler.fit_transform(close_only)

close_scaled

df_date = main_dataframe['Date'].values

df_date

"""Pembagian Validation set sebesar 20% dari total dataset"""

X_date_train, X_date_test, y_close_train, y_close_test = train_test_split(df_date,
                                                                          close_scaled,
                                                                          test_size = 0.2,
                                                                          shuffle = False )

X_date_train

X_date_test

y_close_train

y_close_test

"""Fungsi ini mengubah deret waktu menjadi format yang sesuai untuk melatih model pembelajaran mesin untuk prediksi deret waktu. Ini membuat urutan nilai window_size yang berurutan sebagai fitur masukan dan memasangkan setiap urutan dengan nilai pada langkah waktu berikutnya sebagai variabel target. Fitur masukan dan variabel target yang dihasilkan dikelompokkan ke dalam kumpulan dengan ukuran batch_size, dan urutan kumpulan tersebut diacak. Fungsi ini mengembalikan objek tf.data.Dataset yang dapat digunakan untuk melatih model."""

def win_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  Dataset = tf.data.Dataset.from_tensor_slices(series)
  Dataset = Dataset.window(window_size + 1, shift=1, drop_remainder=True)
  Dataset = Dataset.flat_map(lambda w: w.batch(window_size + 1))
  Dataset = Dataset.shuffle(shuffle_buffer)
  Dataset = Dataset.map(lambda w: (w[:-1], w[-1:]))
  return Dataset.batch(batch_size).prefetch(1)

train_dataset = win_dataset(y_close_train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_dataset = win_dataset(y_close_test, window_size=60, batch_size=100, shuffle_buffer=1000)

train_dataset

"""# Modelling

Model menggunakan Model Sequential
"""

shape_1 = None
shape_2 = 1

model1 = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(128, input_shape=([shape_1, shape_2]), return_sequences = True),
  tf.keras.layers.LSTM(64, return_sequences = True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(16, activation="relu"),
  tf.keras.layers.Dense(1)
])

model1.summary()

"""Optimizer sudah menggunakan learning rate"""

optimize = tf.keras.optimizers.Adam(learning_rate=1.0000e-04)
model1.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimize,
              metrics=["mae"])

"""Batas yang sudah disesuaikan berdasarkan data. berikut adalah threshold untuk MAE sebesar 10% dari skala data"""

threshold_MAE = (close_scaled.max() - close_scaled.min()) * 10/100
print(threshold_MAE)

"""Callback yang akan mengehentikan proses pelatihan jika mencapai batas yang sudah ditentukan. Proses pelatihan akan berhenti jika MAE dan val MAE berada di bawah threshold"""

class EarlyStopper(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('mae') < threshold_MAE and logs.get('val_mae') < threshold_MAE):
          print("MAE and val MAE has reached below the threshold")
          self.model.stop_training = True

EarlyStopper1 = EarlyStopper()

class MaePlot(tf.keras.callbacks.Callback):

    def on_train_begin(self, logs={}):
        self.losses = []
        self.acc = []
        self.val_losses = []
        self.val_acc = []
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):

        self.logs.append(logs)
        self.losses.append(logs.get('loss'))
        self.acc.append(logs.get('mae'))
        self.val_losses.append(logs.get('val_loss'))
        self.val_acc.append(logs.get('val_mae'))

        if len(self.losses) > 1:

            N = np.arange(0, len(self.losses))

            plt.figure()
            plt.plot(N, self.losses, label = "loss")
            plt.plot(N, self.acc, label = "mae")
            plt.plot(N, self.val_losses, label = "val_loss")
            plt.plot(N, self.val_acc, label = "val_mae")
            plt.title("Training Loss and Accuracy [Epoch {}]".format(epoch))
            plt.xlabel("Epoch #")
            plt.ylabel("Loss/MAE")
            plt.legend()
            plt.show()
            plt.close()

MaePlot1 = MaePlot()

model1.fit(train_dataset,
          epochs=100,
          batch_size=100,
          validation_data=val_dataset,
          verbose=2,
          callbacks = [EarlyStopper1, MaePlot1],
          shuffle=False)

"""Proses training model sudah berhasil dan berhenti sesuai dengan Callback yang diatur berdasarkan MAE Threshold sebesar kurang dari 10% skala data dari sebuah data yang memiliki lebih dari 10000 sampel."""